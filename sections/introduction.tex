Unsupervised sequence learning is an important problem in machine learning given that most information (ranging from speech to video to even consumer behavior) is often unlabeled and has a sequential structure. Most of these sequences consist of high-dimensional, complex objects such as words in text, images in video, or chords in music. Recently, recurrent neural networks (RNN) have become state-of-the-art for sequence representation because they have an internal memory that can learn long-term temporal dependencies. 

Maybe I should write this at the end?