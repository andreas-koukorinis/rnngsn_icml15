Unsupervised sequence learning is an important problem in machine learning given that most information (ranging from speech to video to even consumer behavior) is often unlabeled and has a sequential structure. Most of these sequences consist of high-dimensional, complex objects such as words in text, images in video, or chords in music. Recently, recurrent neural networks (RNN) \cite{rnn} have become state-of-the-art for sequence representation because they have an internal memory that can learn long-term, temporal dependencies. Further advances with Hessian-free optimization and Long Short-term Memory for RNNs \cite{hessian_free, lstm} have enabled learning highly complex temporal dependencies.

While traditional RNNs can learn complex sequences through maintaining an internal memory, they don't deal well when modeling complex input distributions, where the conditional distribution at each time step is highly multi-modal. In real-world data, we often care more about this conditional distribution rather than the expected value. In music, for example, the existence of a particular note can highly change the probabilities with which other notes occur at the same time in a chord. In consumer behavior, the existence of a particular action taken can highly change the probabilities of other actions made within the same time window. We would prefer to reason over these distributions at each time step to form a better representation of the sequences.

Deep RNN architectures have been proposed to alleviate the traditional architecture shortcomings with complex input distributions \cite{deep_rnn}. One proposed framework involves using deep input-to-hidden and hidden-to-output functions to reduce input complexity, making the RNN's task easier.

\cite{rnnrbm}

In Sections 2, 3, and 4, we introduce the GSN, RNN, and RNN-GSN architectures. We then provide experimental validation of the RNN-GSN on sequences of MNIST images and midi datasets in Section 5.