Unsupervised sequence learning is an important problem in machine learning given that most information (ranging from speech to video to music to even consumer behavior) is often unlabeled and has a sequential structure. Most of these sequences consist of high-dimensional, complex objects such as words in text, images in video, or chords in music. Recently, recurrent neural networks (RNN) \cite{rnn} have become state-of-the-art for sequence representation because they have an internal memory that can learn long-term, temporal dependencies. Further advances with Hessian-free optimization and Long Short-term Memory for RNNs \cite{hessian_free, lstm} have enabled learning highly complex temporal dependencies.

While traditional RNNs can learn complex sequences through maintaining an internal memory, they don't deal well when modeling complex input distributions, where the conditional distribution at each time step is highly multi-modal. In real-world data, we often care more about this conditional distribution rather than the expected value. In music, for example, the existence of a particular note can greatly change the probabilities with which other notes occur at the same time in a chord. In consumer behavior, the existence of a particular action taken can greatly change the probabilities of other actions made within the same time window. We would prefer to reason and perform inference over these distributions at each time step to form a better representation of the sequences.

Deep RNN architectures have been proposed to alleviate the traditional RNN architecture shortcomings with complex input distributions \cite{deep_rnn}. One proposed framework involves using deep input-to-hidden or hidden-to-output functions to reduce input complexity, making the RNN's temporal modeling task easier. These functions exploit the ability of deep networks to disentangle the underlying factors of variation of the original input and flatten high-density data manifolds \cite{goodfellow09, glorot11, bengio13}.

One such model, the RNN-RBM, replaces the output layer of an RNN with a restricted Boltzmann machine (RBM) to provide a conditional generative model over the input distribution \cite{rnnrbm,boulanger14}. The RNN-RBM is a generalization of previous recurrent temporal RBM work \cite{rtrbm} that has shown promise for modeling complex sequences such as MIDI representations of polyphonic music. Another possible model is to replace the RNN output layer with a neural autoregressive distribution estimator (NADE), which has been shown to provide a tractable distribution estimator that performs similarly to large, intractable RBMs \cite{nade}.

Motivated by the promise of deep RNN models for learning sequence representations, we propose a model using a Generative Stochastic Network (GSN) as the hidden-to-output function of an RNN, called the RNN-GSN. GSNs are a recent generalization of denoising autoencoders \cite{gsn} that are easier to perform inference over compared to RBMs and sample from compared to NADEs. GSNs are non-probabilistic, generative models that have also been shown to learn the same model as a deep orderless NADE, which alleviates the factorization ordering drawback of a traditional NADE model \cite{gsn_nade}.

The rest of this paper explains the RNN-GSN model and provides experimental evidence for its use in modeling complex sequences. In Sections 2, 3, and 4, we introduce the GSN, RNN, and RNN-GSN architectures in more detail. We then provide experimental validation of the RNN-GSN on sequences of MNIST images and standard MIDI datasets in Section 5.