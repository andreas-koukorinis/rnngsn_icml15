Unsupervised sequence learning is an important problem in machine learning given that much data (such as speech, video,  music, and even consumer behavior) is sequential and largely unlabeled. Most of these sequences consist of high-dimensional, complex objects such as words in text, images in video, or chords in music. Recently, recurrent neural networks (RNN) \cite{rnn} have become state-of-the-art for sequence representation because they have an internal memory that can learn long-term temporal dependencies. Further advances with Hessian-free optimization and Long Short-term Memory (LSTM) for RNNs \cite{hessian_free, lstm} have enabled learning of highly complex temporal dependencies.

While traditional RNNs can learn complex sequences by maintaining an internal memory, they don't perform well when modeling complex input distributions, where the conditional distribution at each time step is highly multi-modal. In real-world data, we often care more about this conditional distribution than about the expected value. In music, for example, the existence of a particular note can greatly change the probabilities with which other notes occur with it in a chord. In consumer behavior, the existence of a particular action taken can greatly change the probabilities of other actions made within the same time window. We would prefer to reason and perform inference over these distributions at each time step to form a better representation of the sequences.

Deep RNN architectures have been proposed to alleviate the traditional RNN architecture shortcomings with complex input distributions \cite{deep_rnn}. One proposed framework uses deep input-to-hidden or hidden-to-output functions to reduce input complexity, making the RNN's temporal modeling task easier. These functions exploit the ability of deep networks to disentangle the underlying factors of variation of the original input and flatten high-density data manifolds \cite{goodfellow09, glorot11, bengio13}.

One such model, the RNN-RBM, replaces the output layer of an RNN with a restricted Boltzmann machine (RBM) to provide a conditional generative model over the input distribution \cite{rnnrbm,boulanger14}. The RNN-RBM is a generalization of previous recurrent temporal RBM \cite{rtrbm} that has shown promise for modeling complex sequences such as MIDI representations of polyphonic music. Another possible model, replacing the RNN output layer with a neural autoregressive distribution estimator (NADE),  has been shown to provide a tractable distribution estimator that performs similarly to large, intractable RBMs \cite{nade}.

Motivated by the promise of deep RNN models for learning sequence representations, we propose a new model using a Generative Stochastic Network (GSN) as the hidden-to-output function of an RNN. We call this model the RNN-GSN. GSNs are a recent generalization of denoising autoencoders \cite{gsn} that are easier to perform inference over compared to RBMs and are easier to sample from than NADEs. GSNs are non-probabilistic, generative models that have also been shown to learn the same model as a deep orderless NADE, alleviating the factorization ordering drawback of a traditional NADE model \cite{gsn_nade}.

The rest of this paper explains the RNN-GSN model and provides experimental evidence for its benefit in modeling complex sequences. In Sections 2, 3, and 4, we introduce the GSN, RNN, and RNN-GSN architectures in more detail. We then provide experimental validation of the RNN-GSN on sequences of MNIST images and standard MIDI datasets in Section 5.
