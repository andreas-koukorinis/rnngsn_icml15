We presented the RNN-GSN, a generative, non-probabilistic model for learning deep sequence representations, and validated its efficacy as a deep recurrent model on complex inputs with sequences of MNIST images and MIDI representations of polyphonic music. The RNN-GSN works as well as other deep recurrent models such as the RNN-RBM, but is easier  to perform inference over and to sample from due to its non-probabilistic nature.

The RNN-GSN might be further improvemented by the inclusion of LSTM units in the RNN, the use of Hessian-free optimization, and by combining depth at multiple stages of the RNN. For example, we would like to use GSNs as both the input-to-hidden and hidden-to-output functions of the RNN, further reducing the complexity of representing the sequence. Finally, multimodal GSNs \cite{multi_gsn} could potentially provide benefits when the hidden representations of the input data distribution cannot be assumed to be unimodal.
