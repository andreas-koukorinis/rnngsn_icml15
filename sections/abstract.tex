We present a new generative model for unsupervised learning of sequence representations, the recurrent Generative Stochastic Network (RNN-GSN). This approach builds on recent deep recurrent neural network architectures for modeling high-dimensional, multimodal input distributions, and is able to predict complex sequences of these distributions. The RNN-GSN is non-probabilistic and therefor easier to perform inference over than other models such as the RNN-RBM or NADE variants. We  experimentally validate  this new model on sequences of MNIST images and standard MIDI music datasets.
