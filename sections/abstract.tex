We present a new generative model for unsupervised learning of sequence representations, the recurrent Generative Stochastic Network (RNN-GSN). This approach builds on deep recurrent neural network architectures for modeling high-dimensional, multimodal input distributions and is able to predict complex sequences of these distributions. The RNN-GSN is non-probabilistic and easier to perform inference over compared to other representation models, such as the RNN-RBM or NADE variants.