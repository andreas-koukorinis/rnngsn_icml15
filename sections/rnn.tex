Traditional RNNs simulate a discrete-time system that has an input sequence \(\{x_1...x_t\}\) and a hidden state sequence \(\{h_1...h_t\}\) that map to an output sequence \(\{y_1...y_t\}\). The network is defined for timestep \(t\) in the sequence by the hidden and output equations:
\begin{equation}
	h_t = \Phi_h(W^Th_{t-1} + U^Tx_t + b_h)
\end{equation}
\begin{equation}
	y_t = \Phi_y(V^Th_t + b_y)
\end{equation}
where \(\Phi_h\) and \(\Phi_y\) are element-wise nonlinear functions and \(W\), \(U\), \(V\), \(b_h\), and \(b_y\) are the network parameters \(\Theta\). The hidden state \(h\) contains all information about the sequence for a given timestep \(t\).

RNNs can be trained via backpropagation over a set number of time steps, which is known as backpropagation through time. However, due to the exploding gradient problem, backpropagation through time has difficulty training the parameters for long-term temporal dependencies. To fix the exploding gradient problem, long short-term memory units (LSTM) \cite{lstm} can be introduced as the hidden units that contain extra parameters to learn a gated memory for storing the unit's activation value for a period of time steps. With this architecture, normal backpropagation through time can train the model to learn much longer temporal dependencies. Alternatively, Hessian-free optimization \cite{hessian_free} can learn the model parameters for long-term temporal dependencies without needing to introduce specialized hidden units.

\subsection{Extension to deep RNN}\label{deep_rnn}
While traditional RNNs can be seen as deep networks over multiple timesteps due to the hidden connections \(h_{t-1} \rightarrow h_t\), they suffer from being shallow, meaning they only contain a single nonlinear layer, at each individual timestep. This property limits the complexity of inputs RNNs can realistically represent. When analyzing RNNs in a single timestep, there are four areas where depth can be added: input-to-hidden function \(x_t \rightarrow h_t\), hidden-to-output function \(h_t \rightarrow y_t\), hidden-to-hidden function \(h_{t-1} \rightarrow h_t\), and stacking hidden layers \(h_t\) \cite{deep_rnn}.

Adding depth to the input-to-hidden function \(x_t \rightarrow h_t\) exploits the non-temporal structure of the input. Because higher-level representations tend to better disentangle the underlying factors of variation in the original input \cite{bengio13}, using a deep input-to-hidden function should reduce complexity of the input to the RNN. With a less complex input, the RNN can model the temporal structure between successive timesteps more easily, rather than wasting parameters to also model the input complexity.

Similarly, adding depth to the hidden-to-output function \(h_t \rightarrow y_t\) reduces the complexity of the recurrent hidden state, which makes predicting the output easier. This also allows the hidden state to use less parameters for modeling output complexity, which allows it to devote more resources for modeling temporal structure; the hidden state learns a more efficient temporal representation. This is the structure used by the RNN-RBM/NADE variants, as well as the RNN-GSN used for experiments in Section \ref{experiments}.

A deep hidden-to-hidden transition function \(h_{t-1} \rightarrow h_t\) is slightly different, where multiple layers are added between each timestep. By increasing the number of layers in the hidden states, the RNN can adapt more quickly to changing modes of the input data distribution. However, introducing depth in the recurrent hidden state creates difficulty for performing backpropagation through time, as the gradient has to traverse through more nonlinear steps. This increases the difficulty of capturing longer-term temporal dependencies. The deep hidden-to-hidden transition structure has been used successfully as a recurrent convolutional neural network (RCNN), which uses a convolutional neural network to model the transition between consecutive RNN hidden states \cite{rcnn}.

Finally, RNNs can be made deeper by stacking the hidden layers \(h_t\). This approach differs from deep hidden-to-hidden transitions because each stacked hidden layer would take input from the previous timestep's hidden layers and pass output to the next timestep's hidden layers. This architecture might allow the RNN to represent multiple time scales in the input sequence.

These deeper architectures for RNNs serve the purpose of increasing the efficiency with which the recurrent hidden states $H$ can model the temporal structure of the inputs. We explore adding depth per static frame with the hidden-to-output function, as well as theoretically combining it with a deep input-to-hidden function. This way, the recurrent hidden states can work exclusively on modeling the temporal relationships in the data in a less complex space.