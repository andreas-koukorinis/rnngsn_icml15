Traditional RNNs simulate a discrete-time system that has an input sequence \(\{x_1...x_t\}\) and a hidden state sequence \(\{h_1...h_t\}\) that map to an output sequence \(\{y_1...y_t\}\). The network is defined for timestep \(t\) in the sequence by the hidden and output equations:
\begin{equation}
	h_t = \Phi_h(W^Th_{t-1} + U^Tx_t + b_h)
\end{equation}
\begin{equation}
	y_t = \Phi_y(V^Th_t + b_y)
\end{equation}
where \(\Phi_h\) and \(\Phi_y\) are element-wise nonlinear functions and \(W\), \(U\), \(V\), \(b_h\), and \(b_y\) are the network parameters \(\Theta\). The hidden state \(h\) contains all information about the sequence for a given timestep \(t\).

RNNs can be trained via backpropagation over a set number of time steps, which is known as backpropagation through time. However, due to the exploding gradient problem, backpropagation through time has difficulty training the parameters for long-term temporal dependencies. To fix the exploding gradient problem, long short-term memory units (LSTM) \cite{lstm} can be introduced as the hidden units that contain extra parameters to learn a gated memory for storing the unit's activation value for a period of time steps. With this architecture, normal backpropagation through time can train the model to learn much longer temporal dependencies. Alternatively, Hessian-free optimization \cite{hessian_free} can learn the model parameters for long-term temporal dependencies without needing to introduce specialized hidden units.

\subsection{Extension to deep RNN}
While traditional RNNs can be seen as deep networks over multiple timesteps due to the hidden connections \(h_{t-1} \rightarrow h_t\), they suffer from being shallow, where they only containing a single nonlinear layer, at each individual timestep, which limits the complexity of inputs they can realistically represent. When analyzing RNNs in a single timestep, there are four areas where depth can be added: input-to-hidden function \(x_t \rightarrow h_t\), hidden-to-output function \(h_t \rightarrow y_t\), hidden-to-hidden function \(h_{t-1} \rightarrow h_t\), and stacking hidden layers \(h_t\) \cite{deep_rnn}.

